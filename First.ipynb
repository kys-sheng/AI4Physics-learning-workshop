{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![](https://i.imgur.com/fQY5xFv.png)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Workshop Overview 🎯  \n",
    "\n",
    "### From NLP Breakthroughs to Physics Research 🚀\n",
    "- By NLP-like we mean to learn from breakthroughs in ML for NLP and bring it to physics\n",
    "- The best place to start is symbolic mathematics. \n",
    "\n",
    "#### Expressions are a collection of **terms** (sentences) made up from mathematical **objects** (words) and put together with specific **rule** (syntax) that **encode** information (meaning).\n",
    "---\n",
    "\n",
    "## 🛠️ Part 1: Quickly From Idea to Model\n",
    "- Introducing **Hugging Face** Ecosystem\n",
    "- Rapidly prototype & train state-of-the-art models  \n",
    "- Going from **0 ➡️ 100** with minimal effort\n",
    "\n",
    "---\n",
    "\n",
    "## 🎓 Part 2: Tackling a Realistic Physics Problem\n",
    "- How we approached a practical, interesting scenario\n",
    "- Dataset creation & sampling 🗃️\n",
    "- Tokenization & data preparation 📐\n",
    "- Evaluation & validation of model performance 📈\n",
    "- Computational resources: What and Where? 💻\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Let's import some useful things first. \n",
    "\n",
    "import torch\n",
    "import random\n",
    "import numpy as np\n",
    "from datasets import Dataset\n",
    "# The transformers library is part of the Hugging Face ecosystem and provides a wide range of pre-trained models and tokenizers.\n",
    "from transformers import (\n",
    "    BartTokenizer, \n",
    "    BartForConditionalGeneration, \n",
    "    BartConfig,\n",
    "    Seq2SeqTrainer, \n",
    "    Seq2SeqTrainingArguments,\n",
    "    DataCollatorForSeq2Seq\n",
    ")\n",
    "import wandb\n",
    "\n",
    "\n",
    "# Set best available device. Those of you with macbook MX chips will be very happy to run this on your machine.\n",
    "\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \n",
    "                      \"mps\" if torch.backends.mps.is_available() else \"cpu\")\n",
    "print(f\"Using device: {device}\")\n",
    "\n",
    "\n",
    "# Initialize wandb, which is a tool for tracking experiments and visualizing results.\n",
    "#wandb.init(project=\"addition-bart\", name=\"addition-model\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 🤗 What is Hugging Face?\n",
    "\n",
    "---\n",
    "\n",
    "### 🌐 **Community**\n",
    "- Large, collaborative community of ML researchers and developers\n",
    "- Share, discuss, and learn from others’ work \n",
    "\n",
    "### 📚 **Repository**\n",
    "- Easy access to thousands of pretrained ML models, datasets but also \"empty\" architectures and tools to train them.\n",
    "- Open-source sharing and reproducibility of state-of-the-art models\n",
    "\n",
    "### 🛠️ **Library**\n",
    "- Powerful Python libraries designed to quickly use, train, and fine-tune models\n",
    "- Simplifies experimentation and accelerates research workflows\n",
    "\n",
    "---\n",
    "\n",
    "➡️ **All-in-one ML toolbox:**  \n",
    "Easy to use. Easy to contribute. Easy to innovate."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![](https://i.imgur.com/WU23myI.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# ➕ Addition Task with BART Sequence-to-Sequence Model 🤖\n",
    "\n",
    "---\n",
    "\n",
    "## 🎯 **Goal**\n",
    "- Train a model to perform integer addition  \n",
    "*(e.g., \"5 + 7 = ?\" → \"12\")*\n",
    "\n",
    "## 🧠 **Architecture**\n",
    "- **BART**: Sequence-to-sequence transformer  \n",
    "- Combines:\n",
    "  - Bidirectional encoder (**BERT-like**)\n",
    "  - Autoregressive decoder (**GPT-like**)\n",
    "\n",
    "## ⚙️ **Implementation Steps**\n",
    "- ✅ Configure a small BART model from scratch with Hugging Face\n",
    "- ✅ Generate a custom dataset of addition examples\n",
    "- ✅ Train the model from scratch specifically on this numeric addition task\n",
    "\n",
    "---\n",
    "\n",
    "✨ **Why BART?**  \n",
    "- Ideal structure for sequence-to-sequence tasks\n",
    "- Popular for working with symbolic mathematics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Initialize tokenizer \n",
    "tokenizer = BartTokenizer.from_pretrained(\"facebook/bart-base\")\n",
    "\n",
    "# Define model configuration\n",
    "config = BartConfig(\n",
    "    vocab_size=len(tokenizer),  \n",
    "    d_model=128,\n",
    "    encoder_layers=4,\n",
    "    decoder_layers=4,\n",
    "    encoder_attention_heads=4,\n",
    "    decoder_attention_heads=4,\n",
    "    decoder_ffn_dim=512,\n",
    "    encoder_ffn_dim=512,\n",
    "    max_position_embeddings=32,\n",
    "    bos_token_id=tokenizer.bos_token_id,\n",
    "    eos_token_id=tokenizer.eos_token_id,\n",
    "    pad_token_id=tokenizer.pad_token_id,\n",
    "    decoder_start_token_id=tokenizer.bos_token_id\n",
    ")\n",
    "\n",
    "# Initialize model from scratch with our config\n",
    "model = BartForConditionalGeneration(config)\n",
    "print(f\"Model parameters: {model.num_parameters():,}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 📊 Dataset Generation for Addition Task ➕\n",
    "\n",
    "---\n",
    "\n",
    "## 🎯 **Goal**\n",
    "Generate a dataset of integer addition examples.\n",
    "\n",
    "### 🧩 **Procedure**\n",
    "- 🎲 **Sample integers** randomly from a predefined range.\n",
    "- 🧮 **Compute the sum** for each sampled pair.\n",
    "- 📝 **Format examples clearly** (e.g., `\"23 + 15 = ?\" → \"38\"`).\n",
    "- 📂 **Split data** into training and validation datasets.\n",
    "\n",
    "---\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Generate dataset directly with Hugging Face structures\n",
    "def generate_dataset(num_examples=5000, min_num=1, max_num=100, train_ratio=0.8):\n",
    "    examples = []\n",
    "    for _ in range(num_examples):\n",
    "        num1 = random.randint(min_num, max_num)\n",
    "        num2 = random.randint(min_num, max_num)\n",
    "        examples.append({\n",
    "            \"input_text\": f\"{num1} + {num2} = ?\",\n",
    "            \"target_text\": str(num1 + num2)\n",
    "        })\n",
    "    \n",
    "    # Create and split datasets\n",
    "    dataset = Dataset.from_dict({\n",
    "        \"input_text\": [ex[\"input_text\"] for ex in examples],\n",
    "        \"target_text\": [ex[\"target_text\"] for ex in examples]\n",
    "    })\n",
    "    return dataset.train_test_split(test_size=1-train_ratio, seed=42)\n",
    "\n",
    "# For later to check OOD\n",
    "\n",
    "def check_number_pair_in_dataset(dataset_dict, num1, num2):\n",
    "    # Generate the input text format\n",
    "    input_text = f\"{num1} + {num2} = ?\"\n",
    "    \n",
    "    # Check in the training set\n",
    "    for item in dataset_dict['train']:\n",
    "        if item['input_text'] == input_text:\n",
    "            return True\n",
    "        \n",
    "    # Check in the evaluation set\n",
    "    for item in dataset_dict['test']:\n",
    "        if item['input_text'] == input_text:\n",
    "            return True\n",
    "        \n",
    "    return False\n",
    "\n",
    "# Example usage\n",
    "dataset_dict = generate_dataset()\n",
    "num1, num2 = 4, 5\n",
    "is_in_dataset = check_number_pair_in_dataset(dataset_dict, num1, num2)\n",
    "print(f\"Is '{num1} + {num2} = ?' in the dataset? {'Yes' if is_in_dataset else 'No'}\")\n",
    "\n",
    "\n",
    "# Create datasets\n",
    "dataset_dict = generate_dataset()\n",
    "\n",
    "# Verify data\n",
    "print(\"\\nSample Data:\")\n",
    "for i in range(3):\n",
    "    print(f\"Sample {i+1}: Input: '{dataset_dict['train']['input_text'][i]}', Target: '{dataset_dict['train']['target_text'][i]}'\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 🔢 Data Tokenization for Addition ➕\n",
    "\n",
    "---\n",
    "\n",
    "## 🎯 **What is Tokenization?**\n",
    "Converting input data into numerical representations that models can process.\n",
    "\n",
    "## 🛠️ **Why is it Important?**\n",
    "- NN eat numbers not expressions.\n",
    "- Proper tokenization ensures learning the right patterns.\n",
    "\n",
    "### ✨ **Examples (`73 + 4 = 77`)**\n",
    "- ## **Number-level**:  `\"73 + 4 = 77\"` → `[73, '+', 4, '=', 77]`\n",
    "- ## **Digit-by-digit**:  `\"73 + 4 = 77\"` → `[7, 3, '+', 4, '=', 7, 7]`\n",
    "\n",
    "---\n",
    "\n",
    "## 🚀 **NOW: BART Tokenizer**\n",
    "The Tokenizer used by developers when training the model with Language. Splits texts in subwords. Why? It's good enough."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define preprocessing function - fixed tokenization approach\n",
    "def preprocess_function(examples):\n",
    "    # Tokenize inputs\n",
    "    model_inputs = tokenizer(\n",
    "        examples[\"input_text\"],\n",
    "        padding=\"max_length\",\n",
    "        truncation=True,\n",
    "        max_length=16,\n",
    "        return_tensors=None  # Return python lists\n",
    "    )\n",
    "    \n",
    "    # Tokenize targets\n",
    "    labels = tokenizer(\n",
    "        text_target=examples[\"target_text\"],\n",
    "        padding=\"max_length\",\n",
    "        truncation=True,\n",
    "        max_length=8,\n",
    "        return_tensors=None  # Return python lists\n",
    "    )\n",
    "    \n",
    "    # Replace padding token id with -100 in labels (transformers convention)\n",
    "    for i in range(len(labels[\"input_ids\"])):\n",
    "        labels[\"input_ids\"][i] = [\n",
    "            -100 if token == tokenizer.pad_token_id else token\n",
    "            for token in labels[\"input_ids\"][i]\n",
    "        ]\n",
    "    \n",
    "    model_inputs[\"labels\"] = labels[\"input_ids\"]\n",
    "    return model_inputs\n",
    "\n",
    "# Preprocess datasets\n",
    "tokenized_datasets = dataset_dict.map(\n",
    "    preprocess_function, \n",
    "    batched=True, \n",
    "    remove_columns=[\"input_text\", \"target_text\"]\n",
    ")\n",
    "\n",
    "# Show example tokenization of an input with the actual input and tokenized input\n",
    "print(\"\\nTokenization Example:\")\n",
    "example_input = dataset_dict['train'][0]['input_text']\n",
    "example_tokenized = tokenizer(example_input)\n",
    "print(f\"Input: {example_input}\")\n",
    "print(f\"Tokenized: {example_tokenized}\")\n",
    "# Decode\n",
    "print(f\"Decoded: {tokenizer.decode(example_tokenized['input_ids'])}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# NOW WE TRAIN! \n",
    "\n",
    "### Data Collator\n",
    "The Data Collator is responsible for batching our processed examples together for efficient training. It performs several critical functions:\n",
    "\n",
    "- **Padding**: Ensures all sequences in a batch have the same length by adding padding tokens\n",
    "- **Tensor conversion**: Converts data from Python lists to PyTorch tensors\n",
    "- **Special handling for labels**: Properly masks padded tokens in labels with -100 so they don't contribute to the loss\n",
    "\n",
    "### Training Arguments\n",
    "We configure the training process with parameters like:\n",
    "- Learning rate and optimization settings\n",
    "- Batch sizes and number of epochs\n",
    "- Evaluation and checkpointing frequency\n",
    "- Logging configuration for monitoring training progress\n",
    "\n",
    "After setting up these components, we'll initialize the trainer and start the training process to teach our model how to add numbers!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set up data collator (without using deprecated features)\n",
    "data_collator = DataCollatorForSeq2Seq(\n",
    "    tokenizer=tokenizer,\n",
    "    model=model,\n",
    "    padding=\"max_length\",\n",
    "    max_length=16,\n",
    "    pad_to_multiple_of=8\n",
    ")\n",
    "\n",
    "# Training arguments with better learning rate\n",
    "training_args = Seq2SeqTrainingArguments(\n",
    "    output_dir=\"./addition_results\",\n",
    "    run_name=\"addition-bart-fixed\",\n",
    "    num_train_epochs=20,\n",
    "    per_device_train_batch_size=64,\n",
    "    per_device_eval_batch_size=64,\n",
    "    learning_rate=5e-4,  \n",
    "    report_to=\"none\",\n",
    "    logging_strategy=\"steps\",\n",
    "    logging_steps=100,\n",
    "    predict_with_generate=True,\n",
    "    generation_max_length=8,\n",
    "    warmup_ratio=0.1,\n",
    "    adam_beta1=0.9,\n",
    "    adam_beta2=0.999,\n",
    "    adam_epsilon=1e-8\n",
    ")\n",
    "\n",
    "# Define trainer\n",
    "trainer = Seq2SeqTrainer(\n",
    "    model=model,\n",
    "    args=training_args,\n",
    "    train_dataset=tokenized_datasets[\"train\"],\n",
    "    eval_dataset=tokenized_datasets[\"test\"],\n",
    "    data_collator=data_collator,\n",
    "    tokenizer=tokenizer\n",
    ")\n",
    "\n",
    "# Train the model\n",
    "trainer.train()\n",
    "print(\"Training completed!\")\n",
    "\n",
    "# Save the model\n",
    "model.save_pretrained(\"./addition_model\")\n",
    "\n",
    "# Clean up wandb\n",
    "wandb.finish()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Define test function\n",
    "def test_addition(model, tokenizer, num1, num2):\n",
    "    input_text = f\"{num1} + {num2} = ?\"\n",
    "    inputs = tokenizer(input_text, return_tensors=\"pt\").to(device)\n",
    "    \n",
    "    # Move model to device for prediction\n",
    "    model.to(device)\n",
    "    model.eval()\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        outputs = model.generate(\n",
    "            inputs[\"input_ids\"],\n",
    "            max_length=8,\n",
    "            num_beams=4,\n",
    "            early_stopping=True\n",
    "        )\n",
    "    \n",
    "    prediction = tokenizer.decode(outputs[0], skip_special_tokens=True)\n",
    "    return prediction.strip()\n",
    "\n",
    "# Load best model for testing\n",
    "model = BartForConditionalGeneration.from_pretrained(\"./addition_model\")\n",
    "model.to(device)\n",
    "\n",
    "# Test cases random\n",
    "\n",
    "test_cases = [(random.randint(1, 100), random.randint(1, 100)) for _ in range(10)]\n",
    "\n",
    "print(\"\\nTesting the model:\")\n",
    "\n",
    "# 1 + 1 = 2 example\n",
    "test_cases.append((1, 1))\n",
    "\n",
    "correct = 0\n",
    "for num1, num2 in test_cases:\n",
    "    # Check if in dataset_dict\n",
    "    expected = str(num1 + num2)\n",
    "    predicted = test_addition(model, tokenizer, num1, num2)\n",
    "    print(f\"{num1} + {num2} = {predicted} (Expected: {expected})\")\n",
    "    is_in_dataset = check_number_pair_in_dataset(dataset_dict, num1, num2)\n",
    "    if is_in_dataset:\n",
    "        print(f\"In dataset\")\n",
    "    else:\n",
    "        print(f\"Not in dataset\")\n",
    "    if predicted == expected:\n",
    "        correct += 1\n",
    "\n",
    "print(f\"\\nAccuracy: {correct/len(test_cases):.2%}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# **Beyond a toy problem: Generating Lagrangians**\n",
    "\n",
    "## With this philosophy in mind, we though a good general task to tackle would be:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![](https://i.imgur.com/Tb701DF.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## But as a starting point towards a general Lagrangian generator, we decided to tackle what we know best:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![](https://i.imgur.com/dGfUOPB.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# That's a big jump. What did we have to think to succesfully do NLP-like approach with particle physics Lagrangians?\n",
    "\n",
    "\n",
    "![](https://i.imgur.com/X4NW1q0.png)\n",
    "\n",
    "# link: [https://bit.ly/4ctADR6](https://bit.ly/4ctADR6)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "[](https://imgur.com/dGfUOPB)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "workhorse",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
