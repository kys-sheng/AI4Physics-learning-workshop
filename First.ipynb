{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import random\n",
    "import numpy as np\n",
    "from datasets import Dataset\n",
    "from transformers import (\n",
    "    BartTokenizer, \n",
    "    BartForConditionalGeneration, \n",
    "    BartConfig,\n",
    "    Seq2SeqTrainer, \n",
    "    Seq2SeqTrainingArguments,\n",
    "    DataCollatorForSeq2Seq\n",
    ")\n",
    "import wandb\n",
    "\n",
    "\n",
    "# Set best available device\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \n",
    "                      \"mps\" if torch.backends.mps.is_available() else \"cpu\")\n",
    "print(f\"Using device: {device}\")\n",
    "\n",
    "\n",
    "# Initialize wandb\n",
    "wandb.init(project=\"addition-bart\", name=\"fixed-addition-model\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Addition Task with BART Sequence-to-Sequence Model\n",
    "\n",
    "This project demonstrates:\n",
    "\n",
    "1. **Task**: Building a model that adds two integers (e.g., \"5 + 7 = ?\" â†’ \"12\")\n",
    "2. **Architecture**: Using BART, a pre-trained sequence-to-sequence transformer model\n",
    "3. **Implementation**:\n",
    "    - Configuring a smaller BART model from Hugging Face\n",
    "    - Creating a custom dataset of addition problems\n",
    "    - Fine-tuning the model on this mathematical task\n",
    "\n",
    "BART combines a bidirectional encoder (like BERT) with an autoregressive decoder (like GPT), making it well-suited for various sequence transformation tasks including our numeric addition problem."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Initialize tokenizer \n",
    "tokenizer = BartTokenizer.from_pretrained(\"facebook/bart-base\")\n",
    "\n",
    "# Define model configuration\n",
    "config = BartConfig(\n",
    "    vocab_size=len(tokenizer),  # Important: match with expanded tokenizer\n",
    "    d_model=128,\n",
    "    encoder_layers=3,\n",
    "    decoder_layers=3,\n",
    "    encoder_attention_heads=2,\n",
    "    decoder_attention_heads=2,\n",
    "    decoder_ffn_dim=512,\n",
    "    encoder_ffn_dim=512,\n",
    "    max_position_embeddings=64,\n",
    "    bos_token_id=tokenizer.bos_token_id,\n",
    "    eos_token_id=tokenizer.eos_token_id,\n",
    "    pad_token_id=tokenizer.pad_token_id,\n",
    "    decoder_start_token_id=tokenizer.bos_token_id\n",
    ")\n",
    "\n",
    "# Initialize model from scratch with our config\n",
    "model = BartForConditionalGeneration(config)\n",
    "print(f\"Model parameters: {model.num_parameters():,}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Dataset Generation\n",
    "\n",
    "We'll create synthetic physics data by:\n",
    "- Sampling input parameters from appropriate distributions\n",
    "- Computing outputs using known physical relationships\n",
    "- Adding noise to simulate measurement uncertainty\n",
    "- Splitting into training and validation sets\n",
    "\n",
    "This data will help us evaluate our model's ability to learn the underlying physical laws."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Generate dataset directly with Hugging Face structures\n",
    "def generate_dataset(num_examples=5000, min_num=1, max_num=100, train_ratio=0.8):\n",
    "    examples = []\n",
    "    for _ in range(num_examples):\n",
    "        num1 = random.randint(min_num, max_num)\n",
    "        num2 = random.randint(min_num, max_num)\n",
    "        examples.append({\n",
    "            \"input_text\": f\"{num1} + {num2} = ?\",\n",
    "            \"target_text\": str(num1 + num2)\n",
    "        })\n",
    "    \n",
    "    # Create and split datasets\n",
    "    dataset = Dataset.from_dict({\n",
    "        \"input_text\": [ex[\"input_text\"] for ex in examples],\n",
    "        \"target_text\": [ex[\"target_text\"] for ex in examples]\n",
    "    })\n",
    "    return dataset.train_test_split(test_size=1-train_ratio, seed=42)\n",
    "\n",
    "\n",
    "# Create datasets\n",
    "dataset_dict = generate_dataset()\n",
    "\n",
    "# Verify data\n",
    "print(\"\\nSample Data:\")\n",
    "for i in range(3):\n",
    "    print(f\"Sample {i+1}: Input: '{dataset_dict['train']['input_text'][i]}', Target: '{dataset_dict['train']['target_text'][i]}'\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data Tokenization\n",
    "\n",
    "For our addition model to process the data effectively:\n",
    "\n",
    "- We'll convert our numerical inputs into a format suitable for the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define preprocessing function - fixed tokenization approach\n",
    "def preprocess_function(examples):\n",
    "    # Tokenize inputs\n",
    "    model_inputs = tokenizer(\n",
    "        examples[\"input_text\"],\n",
    "        padding=\"max_length\",\n",
    "        truncation=True,\n",
    "        max_length=16,\n",
    "        return_tensors=None  # Return python lists\n",
    "    )\n",
    "    \n",
    "    # Tokenize targets\n",
    "    labels = tokenizer(\n",
    "        text_target=examples[\"target_text\"],\n",
    "        padding=\"max_length\",\n",
    "        truncation=True,\n",
    "        max_length=8,\n",
    "        return_tensors=None  # Return python lists\n",
    "    )\n",
    "    \n",
    "    # Replace padding token id with -100 in labels (transformers convention)\n",
    "    for i in range(len(labels[\"input_ids\"])):\n",
    "        labels[\"input_ids\"][i] = [\n",
    "            -100 if token == tokenizer.pad_token_id else token\n",
    "            for token in labels[\"input_ids\"][i]\n",
    "        ]\n",
    "    \n",
    "    model_inputs[\"labels\"] = labels[\"input_ids\"]\n",
    "    return model_inputs\n",
    "\n",
    "# Preprocess datasets\n",
    "tokenized_datasets = dataset_dict.map(\n",
    "    preprocess_function, \n",
    "    batched=True, \n",
    "    remove_columns=[\"input_text\", \"target_text\"]\n",
    ")\n",
    "\n",
    "# Verify tokenized data\n",
    "print(\"\\nVerifying tokenized data:\")\n",
    "print(f\"Input_ids sample: {tokenized_datasets['train'][0]['input_ids'][:10]}\")\n",
    "print(f\"Labels sample: {tokenized_datasets['train'][0]['labels'][:10]}\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# FINALLY WE TRAIN! \n",
    "\n",
    "### Data Collator\n",
    "The Data Collator is responsible for batching our processed examples together for efficient training. It performs several critical functions:\n",
    "\n",
    "- **Padding**: Ensures all sequences in a batch have the same length by adding padding tokens\n",
    "- **Tensor conversion**: Converts data from Python lists to PyTorch tensors\n",
    "- **Special handling for labels**: Properly masks padded tokens in labels with -100 so they don't contribute to the loss\n",
    "\n",
    "### Training Arguments\n",
    "We configure the training process with parameters like:\n",
    "- Learning rate and optimization settings\n",
    "- Batch sizes and number of epochs\n",
    "- Evaluation and checkpointing frequency\n",
    "- Logging configuration for monitoring training progress\n",
    "\n",
    "After setting up these components, we'll initialize the trainer and start the training process to teach our model how to add numbers!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set up data collator (without using deprecated features)\n",
    "data_collator = DataCollatorForSeq2Seq(\n",
    "    tokenizer=tokenizer,\n",
    "    model=model,\n",
    "    padding=\"max_length\",\n",
    "    max_length=16,\n",
    "    pad_to_multiple_of=8\n",
    ")\n",
    "\n",
    "# Training arguments with better learning rate\n",
    "training_args = Seq2SeqTrainingArguments(\n",
    "    output_dir=\"./addition_results\",\n",
    "    run_name=\"addition-bart-fixed\",\n",
    "    num_train_epochs=20,\n",
    "    per_device_train_batch_size=64,\n",
    "    per_device_eval_batch_size=64,\n",
    "    learning_rate=5e-4,  \n",
    "    weight_decay=0.01,\n",
    "    eval_strategy=\"epoch\",\n",
    "    save_strategy=\"epoch\",\n",
    "    save_total_limit=2,\n",
    "    load_best_model_at_end=True,\n",
    "    report_to=\"wandb\",\n",
    "    logging_strategy=\"steps\",\n",
    "    logging_steps=100,\n",
    "    predict_with_generate=True,\n",
    "    generation_max_length=8,\n",
    "    warmup_ratio=0.1,\n",
    "    adam_beta1=0.9,\n",
    "    adam_beta2=0.999,\n",
    "    adam_epsilon=1e-8\n",
    ")\n",
    "\n",
    "# Define trainer\n",
    "trainer = Seq2SeqTrainer(\n",
    "    model=model,\n",
    "    args=training_args,\n",
    "    train_dataset=tokenized_datasets[\"train\"],\n",
    "    eval_dataset=tokenized_datasets[\"test\"],\n",
    "    data_collator=data_collator,\n",
    "    tokenizer=tokenizer\n",
    ")\n",
    "\n",
    "# Train the model\n",
    "trainer.train()\n",
    "print(\"Training completed!\")\n",
    "\n",
    "# Save the model\n",
    "model.save_pretrained(\"./addition_model\")\n",
    "\n",
    "# Clean up wandb\n",
    "wandb.finish()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Define test function\n",
    "def test_addition(model, tokenizer, num1, num2):\n",
    "    input_text = f\"{num1} + {num2} = ?\"\n",
    "    inputs = tokenizer(input_text, return_tensors=\"pt\").to(device)\n",
    "    \n",
    "    # Move model to device for prediction\n",
    "    model.to(device)\n",
    "    model.eval()\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        outputs = model.generate(\n",
    "            inputs[\"input_ids\"],\n",
    "            max_length=8,\n",
    "            num_beams=4,\n",
    "            early_stopping=True\n",
    "        )\n",
    "    \n",
    "    prediction = tokenizer.decode(outputs[0], skip_special_tokens=True)\n",
    "    return prediction.strip()\n",
    "\n",
    "# Load best model for testing\n",
    "model = BartForConditionalGeneration.from_pretrained(\"./addition_model\")\n",
    "model.to(device)\n",
    "\n",
    "# Test cases random\n",
    "\n",
    "test_cases = [(random.randint(1, 100), random.randint(1, 100)) for _ in range(10)]\n",
    "\n",
    "print(\"\\nTesting the model:\")\n",
    "correct = 0\n",
    "for num1, num2 in test_cases:\n",
    "    # Check if in dataset_dict\n",
    "\n",
    "    expected = str(num1 + num2)\n",
    "    predicted = test_addition(model, tokenizer, num1, num2)\n",
    "    print(f\"{num1} + {num2} = {predicted} (Expected: {expected})\")\n",
    "    if predicted == expected:\n",
    "        correct += 1\n",
    "\n",
    "print(f\"\\nAccuracy: {correct/len(test_cases):.2%}\")\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "workhorse",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
