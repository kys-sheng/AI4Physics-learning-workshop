{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h1> How to do NLP-like research in physics\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This notebook provides a step-by-step demonstration/tutorial based on the Lagrangian paper."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Acknowledge SUPR"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The computations and data handling were enabled by resources provided by the National Academic Infrastructure for Supercomputing in Sweden (NAISS) from projects ????, partially funded by the Swedish Research Council through grant agreement no. 2022-06725"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Introduction\n",
    "A short flash-talk style introduction to the Lagrangian paper to ensure we are on the same page regarding the example."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Link to slides: $\\texttt{www.something.com}$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Models\n",
    "- Overview of HuggingFace library.\n",
    "- How to find off-the-shelf transformer models (e.g., BART-L).\n",
    "- Example usage of a HuggingFace model."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## HuggingFace Library"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import HuggingFace libraries\n",
    "from transformers import AutoModelForSeq2SeqLM, AutoTokenizer\n",
    "\n",
    "# Load a pre-trained model and tokenizer (e.g., BART-Large)\n",
    "model_name = 'facebook/bart-large'\n",
    "model = AutoModelForSeq2SeqLM.from_pretrained(model_name)\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
    "\n",
    "# Example usage\n",
    "text = \"This is a sample input.\"\n",
    "inputs = tokenizer(text, return_tensors=\"pt\")\n",
    "outputs = model.generate(**inputs)\n",
    "print(tokenizer.decode(outputs[0], skip_special_tokens=True))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import BartForConditionalGeneration, PreTrainedTokenizerFast\n",
    "\n",
    "model_name = \"JoseEliel/BART-Lagrangian\"\n",
    "model = BartForConditionalGeneration.from_pretrained(model_name)\n",
    "hf_tokenizer = PreTrainedTokenizerFast.from_pretrained(model_name)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Dataset\n",
    "- Discussion on data generation considerations:\n",
    "  - Data distribution.\n",
    "  - Tokenization choices.\n",
    "- Example of tokenizing a dataset."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data Distribution\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Show plots from paper:\n",
    "- one from random ->  more equal better at long expression\n",
    "- one from smart  ->  more biased (cover edge terms) better at special cases"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Tokenization choices\n",
    "Considerations: \n",
    "- What information is required for your model to learn?\n",
    "- Do you care about expressivity? \n",
    "\n",
    "Practical \n",
    "- How much "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Example: Tokenizing a dataset\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Example: Tokenizing a dataset\n",
    "dataset = [\"Example sentence 1.\", \"Example sentence 2.\"]\n",
    "tokenized_dataset = [tokenizer(sentence, return_tensors=\"pt\") for sentence in dataset]\n",
    "print(tokenized_dataset)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Training\n",
    "- Mention available resources: SUPR/NAISS -> Alvis.\n",
    "- Example of training a model."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##  Mention available resources: SUPR/NAISS -> Alvis.\n",
    "How to access ALVIS\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## CPU or GPU"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# DO you have GPU?\n",
    "\n",
    "# set the device to GPU if available\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "# move the model to the device\n",
    "model.to(device)\n",
    "# Example usage with GPU\n",
    "text = \"This is a sample input.\"\n",
    "inputs = tokenizer(text, return_tensors=\"pt\").to(device)\n",
    "outputs = model.generate(**inputs)\n",
    "print(tokenizer.decode(outputs[0], skip_special_tokens=True))\n",
    "# Example usage with GPU\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Example: Training a model (pseudo-code)\n",
    "# Define training loop and optimizer\n",
    "from transformers import AdamW\n",
    "\n",
    "optimizer = AdamW(model.parameters(), lr=5e-5)\n",
    "for epoch in range(3):\n",
    "    for batch in tokenized_dataset:\n",
    "        outputs = model(**batch, labels=batch['input_ids'])\n",
    "        loss = outputs.loss\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        optimizer.zero_grad()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Evaluation\n",
    "- Generating output from the model.\n",
    "- Discussion on evaluation choices:\n",
    "  - Existing or novel metrics.\n",
    "  - Embedding analysis.\n",
    "  - Out-of-distribution tests."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Example: Generating output\n",
    "test_text = \"This is a test input.\"\n",
    "test_inputs = tokenizer(test_text, return_tensors=\"pt\")\n",
    "test_outputs = model.generate(**test_inputs)\n",
    "print(tokenizer.decode(test_outputs[0], skip_special_tokens=True))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Existing Metric  : Does it work? "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "mainly to see if things work as expected\n",
    "Loss : Deviation from actual term \n",
    "Accuracy : How much is perfect? \n",
    "New metric, Score : (Order does not always matter, XEN)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Embedding analysis : What has it really learn?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Considerations : \n",
    "- Is efficiency the only think you need? \n",
    "- Or is it important for you to know whether the model knows what it is learning? \n",
    "\n",
    "Practical Questions : \n",
    "- Can it associate inputs to some embedding space? <br> \n",
    "- Can it understand relations between inputs?  <br> "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## OOD Generalization : Can it go beyond what its trained? "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Considerations : \n",
    "- Is your problem's \"data space\" very big? \n",
    "- Is the probably of an unseen case high? \n",
    "- If yes, then chances of OOD data cases are high. \n",
    "- Do you want to think about the next archietcture?\n",
    "\n",
    "Practical Questions : \n",
    "- Can it work with never seen scenarios? What is your OOD?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using device: mps\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/eliel/opt/anaconda3/envs/workhorse/lib/python3.9/site-packages/transformers/training_args.py:1575: FutureWarning: `evaluation_strategy` is deprecated and will be removed in version 4.46 of ðŸ¤— Transformers. Use `eval_strategy` instead\n",
      "  warnings.warn(\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m The `run_name` is currently set to the same value as `TrainingArguments.output_dir`. If this was not intended, please specify a different run name by setting the `TrainingArguments.run_name` parameter.\n",
      "Failed to detect the name of this notebook, you can set it manually with the WANDB_NOTEBOOK_NAME environment variable to enable code saving.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Starting training...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[34m\u001b[1mwandb\u001b[0m: Currently logged in as: \u001b[33mkysheng\u001b[0m (\u001b[33mml-thep\u001b[0m). Use \u001b[1m`wandb login --relogin`\u001b[0m to force relogin\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "wandb version 0.19.9 is available!  To upgrade, please run:\n",
       " $ pip install wandb --upgrade"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Tracking run with wandb version 0.16.3"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Run data is saved locally in <code>/Users/eliel/Dropbox/ML4MB/AI4Physics-learning-workshop/wandb/run-20250407_174408-2apsl05r</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Syncing run <strong><a href='https://wandb.ai/ml-thep/huggingface/runs/2apsl05r' target=\"_blank\">output</a></strong> to <a href='https://wandb.ai/ml-thep/huggingface' target=\"_blank\">Weights & Biases</a> (<a href='https://wandb.me/run' target=\"_blank\">docs</a>)<br/>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View project at <a href='https://wandb.ai/ml-thep/huggingface' target=\"_blank\">https://wandb.ai/ml-thep/huggingface</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run at <a href='https://wandb.ai/ml-thep/huggingface/runs/2apsl05r' target=\"_blank\">https://wandb.ai/ml-thep/huggingface/runs/2apsl05r</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/eliel/opt/anaconda3/envs/workhorse/lib/python3.9/site-packages/transformers/data/data_collator.py:657: UserWarning: Creating a tensor from a list of numpy.ndarrays is extremely slow. Please consider converting the list to a single numpy.ndarray with numpy.array() before converting to a tensor. (Triggered internally at /Users/runner/work/pytorch/pytorch/pytorch/torch/csrc/utils/tensor_new.cpp:257.)\n",
      "  batch[\"labels\"] = torch.tensor(batch[\"labels\"], dtype=torch.int64)\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='7000' max='7000' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [7000/7000 06:17, Epoch 1000/1000]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Step</th>\n",
       "      <th>Training Loss</th>\n",
       "      <th>Validation Loss</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>500</td>\n",
       "      <td>8.952300</td>\n",
       "      <td>8.984289</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1000</td>\n",
       "      <td>7.312700</td>\n",
       "      <td>7.476482</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1500</td>\n",
       "      <td>5.840000</td>\n",
       "      <td>6.211458</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2000</td>\n",
       "      <td>4.650700</td>\n",
       "      <td>5.179939</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2500</td>\n",
       "      <td>3.807700</td>\n",
       "      <td>4.451714</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3000</td>\n",
       "      <td>3.261900</td>\n",
       "      <td>4.003561</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3500</td>\n",
       "      <td>2.947000</td>\n",
       "      <td>3.640288</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4000</td>\n",
       "      <td>2.724800</td>\n",
       "      <td>3.361407</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4500</td>\n",
       "      <td>2.557800</td>\n",
       "      <td>3.166435</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>5000</td>\n",
       "      <td>2.442100</td>\n",
       "      <td>3.026706</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>5500</td>\n",
       "      <td>2.348700</td>\n",
       "      <td>2.923602</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>6000</td>\n",
       "      <td>2.288700</td>\n",
       "      <td>2.855395</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>6500</td>\n",
       "      <td>2.250000</td>\n",
       "      <td>2.814618</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>7000</td>\n",
       "      <td>2.244000</td>\n",
       "      <td>2.801322</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='2' max='2' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [2/2 00:00]\n",
       "    </div>\n",
       "    "
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Running tests...\n",
      "Testing: 07+25=\n",
      "Predicted: '', Actual: '032', Correct: False\n",
      "Testing: 45+55=\n",
      "Predicted: '', Actual: '100', Correct: False\n",
      "Testing: 99+01=\n",
      "Predicted: '', Actual: '100', Correct: False\n"
     ]
    }
   ],
   "source": [
    "#!/usr/bin/env python3\n",
    "\"\"\"\n",
    "A simplified example of training a small BART model to perform addition,\n",
    "using HuggingFace components and with proper device support (CPU/CUDA/MPS).\n",
    "\"\"\"\n",
    "\n",
    "import torch\n",
    "import random\n",
    "import numpy as np\n",
    "from torch.utils.data import Dataset\n",
    "from transformers import (\n",
    "    BartTokenizer,\n",
    "    BartConfig,\n",
    "    BartForConditionalGeneration,\n",
    "    Trainer,\n",
    "    TrainingArguments,\n",
    "    DataCollatorForSeq2Seq,\n",
    ")\n",
    "\n",
    "# Set random seeds for reproducibility\n",
    "random.seed(42)\n",
    "np.random.seed(42)\n",
    "torch.manual_seed(42)\n",
    "\n",
    "# Select appropriate device (CPU, CUDA, or MPS)\n",
    "if torch.backends.mps.is_available():\n",
    "    device = torch.device(\"mps\")\n",
    "elif torch.cuda.is_available():\n",
    "    device = torch.device(\"cuda\")\n",
    "else:\n",
    "    device = torch.device(\"cpu\")\n",
    "print(f\"Using device: {device}\")\n",
    "\n",
    "# Load the pretrained tokenizer\n",
    "tokenizer = BartTokenizer.from_pretrained(\"facebook/bart-base\")\n",
    "\n",
    "class AdditionDataset(Dataset):\n",
    "    def __init__(self, num_samples, tokenizer):\n",
    "        self.examples = []\n",
    "        for _ in range(num_samples):\n",
    "            a = random.randint(0, 99)\n",
    "            b = random.randint(0, 99)\n",
    "            inp = f\"{a:02d}+{b:02d}=\"\n",
    "            target = f\"{a + b:03d}\"\n",
    "            self.examples.append((inp, target))\n",
    "    \n",
    "    def __getitem__(self, idx):\n",
    "        inp, target = self.examples[idx]\n",
    "        # Tokenize inputs\n",
    "        model_inputs = tokenizer(\n",
    "            inp, \n",
    "            max_length=6,\n",
    "            padding=\"max_length\",\n",
    "            truncation=True,\n",
    "            return_tensors=\"pt\"\n",
    "        )\n",
    "        \n",
    "        # Tokenize targets using the text_target argument (adding special tokens)\n",
    "        labels = tokenizer(\n",
    "            text_target=target,\n",
    "            max_length=4,  # Allows for digit tokens plus an EOS token\n",
    "            padding=\"max_length\",\n",
    "            truncation=True,\n",
    "            return_tensors=\"pt\"\n",
    "        )[\"input_ids\"].squeeze(0)\n",
    "        \n",
    "        # Replace pad token ids with -100 (ignore in loss computation)\n",
    "        labels[labels == tokenizer.pad_token_id] = -100\n",
    "        \n",
    "        # Squeeze the input tensor dimensions (from [1, ...] to [...])\n",
    "        model_inputs = {k: v.squeeze(0) for k, v in model_inputs.items()}\n",
    "        model_inputs[\"labels\"] = labels\n",
    "        return model_inputs\n",
    "    \n",
    "    def __len__(self):\n",
    "        return len(self.examples)\n",
    "\n",
    "# Create training and validation datasets\n",
    "train_dataset = AdditionDataset(num_samples=400, tokenizer=tokenizer)\n",
    "val_dataset = AdditionDataset(num_samples=100, tokenizer=tokenizer)\n",
    "\n",
    "# Define a small BART configuration with the proper special token settings.\n",
    "config = BartConfig(\n",
    "    vocab_size=tokenizer.vocab_size,\n",
    "    max_position_embeddings=32,\n",
    "    encoder_layers=2,\n",
    "    decoder_layers=2,\n",
    "    encoder_attention_heads=2,\n",
    "    decoder_attention_heads=2,\n",
    "    encoder_ffn_dim=64,\n",
    "    decoder_ffn_dim=64,\n",
    "    d_model=32,\n",
    "    activation_function=\"gelu\",\n",
    "    dropout=0.1,\n",
    "    bos_token_id=tokenizer.bos_token_id,  # beginning-of-sequence token\n",
    "    eos_token_id=tokenizer.eos_token_id,  # end-of-sequence token\n",
    "    pad_token_id=tokenizer.pad_token_id,\n",
    ")\n",
    "# IMPORTANT: set decoder_start_token_id so the generation is primed correctly.\n",
    "config.decoder_start_token_id = tokenizer.bos_token_id\n",
    "\n",
    "# Initialize the model and move it to the selected device\n",
    "model = BartForConditionalGeneration(config)\n",
    "model.to(device)\n",
    "\n",
    "# Compute steps per epoch. With 400 samples and a batch size of 8, that's 50 steps per epoch.\n",
    "steps_per_epoch = len(train_dataset) // 8  # 400 // 8 = 50\n",
    "\n",
    "# We want evaluation to happen every 10 epochs, i.e. every 50 * 10 = 500 steps.\n",
    "eval_interval = steps_per_epoch * 10\n",
    "\n",
    "# Set up training arguments. Using evaluation_strategy=\"steps\" with eval_steps=500\n",
    "# will print the evaluation table every 500 steps (i.e., every 10 epochs).\n",
    "training_args = TrainingArguments(\n",
    "    output_dir=\"output\",\n",
    "    evaluation_strategy=\"steps\",\n",
    "    eval_steps=eval_interval,\n",
    "    num_train_epochs=1000,\n",
    "    per_device_train_batch_size=64,\n",
    "    per_device_eval_batch_size=64,\n",
    "    logging_steps=10,\n",
    "    save_strategy=\"epoch\",\n",
    "    save_total_limit=2,\n",
    ")\n",
    "\n",
    "# Initialize the Trainer\n",
    "trainer = Trainer(\n",
    "    model=model,\n",
    "    args=training_args,\n",
    "    train_dataset=train_dataset,\n",
    "    eval_dataset=val_dataset,\n",
    "    data_collator=DataCollatorForSeq2Seq(tokenizer, model=model),\n",
    ")\n",
    "\n",
    "# Train the model\n",
    "print(\"Starting training...\")\n",
    "trainer.train()\n",
    "trainer.evaluate()  # This will do a final evaluation after training\n",
    "\n",
    "\n",
    "# Run tests\n",
    "print(\"\\nRunning tests...\")\n",
    "test_addition(7, 25)    # Expected: 032\n",
    "test_addition(45, 55)   # Expected: 100\n",
    "test_addition(99, 1)    # Expected: 100"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "workhorse",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
