{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Lagrangian Paper Tutorial\n",
    "This notebook provides a step-by-step demonstration/tutorial based on the Lagrangian paper."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Introduction\n",
    "A short flash-talk style introduction to the Lagrangian paper to ensure we are on the same page regarding the example."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Models\n",
    "- Overview of HuggingFace library.\n",
    "- How to find off-the-shelf transformer models (e.g., BART-L).\n",
    "- Example usage of a HuggingFace model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import HuggingFace libraries\n",
    "from transformers import AutoModelForSeq2SeqLM, AutoTokenizer\n",
    "\n",
    "# Load a pre-trained model and tokenizer (e.g., BART-Large)\n",
    "model_name = 'facebook/bart-large'\n",
    "model = AutoModelForSeq2SeqLM.from_pretrained(model_name)\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
    "\n",
    "# Example usage\n",
    "text = \"This is a sample input.\"\n",
    "inputs = tokenizer(text, return_tensors=\"pt\")\n",
    "outputs = model.generate(**inputs)\n",
    "print(tokenizer.decode(outputs[0], skip_special_tokens=True))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import BartForConditionalGeneration, PreTrainedTokenizerFast\n",
    "\n",
    "model_name = \"JoseEliel/BART-Lagrangian\"\n",
    "model = BartForConditionalGeneration.from_pretrained(model_name)\n",
    "hf_tokenizer = PreTrainedTokenizerFast.from_pretrained(model_name)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Dataset\n",
    "- Discussion on data generation considerations:\n",
    "  - Data distribution.\n",
    "  - Tokenization choices.\n",
    "- Example of tokenizing a dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Example: Tokenizing a dataset\n",
    "dataset = [\"Example sentence 1.\", \"Example sentence 2.\"]\n",
    "tokenized_dataset = [tokenizer(sentence, return_tensors=\"pt\") for sentence in dataset]\n",
    "print(tokenized_dataset)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Training\n",
    "- Mention available resources: SUPR/NAISS -> Alvis.\n",
    "- Example of training a model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Example: Training a model (pseudo-code)\n",
    "# Define training loop and optimizer\n",
    "from transformers import AdamW\n",
    "\n",
    "optimizer = AdamW(model.parameters(), lr=5e-5)\n",
    "for epoch in range(3):\n",
    "    for batch in tokenized_dataset:\n",
    "        outputs = model(**batch, labels=batch['input_ids'])\n",
    "        loss = outputs.loss\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        optimizer.zero_grad()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Evaluation\n",
    "- Generating output from the model.\n",
    "- Discussion on evaluation choices:\n",
    "  - Existing or novel metrics.\n",
    "  - Embedding analysis.\n",
    "  - Out-of-distribution tests."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Example: Generating output\n",
    "test_text = \"This is a test input.\"\n",
    "test_inputs = tokenizer(test_text, return_tensors=\"pt\")\n",
    "test_outputs = model.generate(**test_inputs)\n",
    "print(tokenizer.decode(test_outputs[0], skip_special_tokens=True))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
